{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной проекте мной решалась задача классификации\n",
    "\n",
    "На основе градиентного бустинга была построена модель, которая определяет, является ли комментарий позитивным или негативным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка <a id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: spacy in c:\\anaconda3\\envs\\practicum_1\\lib\\site-packages (3.5.2)\n",
      "Collecting download\n",
      "  Using cached download-0.3.5-py3-none-any.whl (8.8 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement en (from versions: none)\n",
      "ERROR: No matching distribution found for en\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Кимон\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Кимон\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     GridSearchCV,\n",
    "                                     StratifiedKFold)\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('../datasets/toxic_comments.csv1')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий анализ текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что есть два столбца. Текст и оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим что классы несбалансированы. Но мы будем избавляться от дисбаланса путём передачи нужного аргумента в модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию с лемматизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemma for each token and join\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z']\", ' ', text) \n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим несколько предложений до лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 159292/159292 [32:13<00:00, 82.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 32min 19s\n",
      "Wall time: 32min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].apply(clear_text)\n",
    "data['lemm_text'] = data['text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text']  = data['lemm_text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now\",\n",
       " 0,\n",
       " 'explanation why the edit make under my username hardcore metallica fan be revert they be not vandalism just closure on some gas after I vote at new york dolls fac and please do not remove the template from the talk page since I be retire now']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого лемматизация прошла успешно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение на обучающую и тестовые выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем пользоваться кросс-валидацией, так что возьмём только 10 % данных для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data[['lemm_text']]\n",
    "target = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# передаём stratify, так как классы несбалансированы\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(\n",
    "        feature, target, test_size=0.1, stratify = target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание Pipeline, который трансформирует текст\n",
    "text_transformer = Pipeline(steps=[\n",
    "    #('lemma', FunctionTransformer(lemmatize_df)),\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
    "])\n",
    "\n",
    "# создание Columntransform, чтобы применить преобразование к одному столбцу\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_transformer', text_transformer, 'lemm_text'),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как классы несбалансированы будет передавать StratifiedKFold в cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = LogisticRegression(random_state = RANDOM_STATE,\n",
    "                    class_weight = 'balanced')\n",
    "\n",
    "steps = [\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('rc', rc)\n",
    "]\n",
    "\n",
    "pipe_rc = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'rc__alpha' : [0.1, 0.5, 1, 10]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rc = GridSearchCV(pipe_rc,\n",
    "                      param_grid ={},\n",
    "                      cv = skf,\n",
    "                      scoring='f1',\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=  12.4s\n",
      "[CV] END .................................................... total time=   9.6s\n",
      "[CV] END .................................................... total time=   9.8s\n",
      "CPU times: total: 40.5 s\n",
      "Wall time: 43.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7499298596338758"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_rc.fit(feature_train, target_train) \n",
    "grid_rc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Итого f1 = 0.75. Модель логистической регрессии прошла проверку\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LGBMClassifier(random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('lg', lg)\n",
    "]\n",
    "pipe_lg = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lg = GridSearchCV(pipe_lg,\n",
    "                      param_grid ={},\n",
    "                      cv = skf,\n",
    "                      scoring='f1',\n",
    "                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=  27.9s\n",
      "[CV] END .................................................... total time=  27.0s\n",
      "[CV] END .................................................... total time=  27.2s\n",
      "CPU times: total: 11min 57s\n",
      "Wall time: 2min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7523612765060813"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_lg.fit(feature_train, target_train) \n",
    "grid_lg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак нужная метрика достигнута"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7561327561327562"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lg.score(feature_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель подтвердила свою эффективность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Нужной метрики удалось добиться на LightGBM. В целом модель достаточно быстро обучается и лучше вариант сложно будет найти\n",
    " - Линейная регрессия показала себя плохо. Скорее всего стоило брать модель без регуляризации, тогда бы показатели могли быть лучше\n",
    " - Лемматизацию нельзя применять ко всему предложению, а надо применять только к словам. Сами предложения будут обрабатываться очень долго"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
